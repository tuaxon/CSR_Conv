{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "from torch.utils.data import Dataset, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_results_exp = []\n",
    "test_accs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "batch_size=128\n",
    "workers=8\n",
    "lr=0.05\n",
    "momentum=0.9\n",
    "weight_decay=5e-4\n",
    "print_freq=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 2 every 30 epochs\"\"\"\n",
    "    lrt = lr * (0.5 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10(root='data', train=True, transform=transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(32, 4),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]), download=True),\n",
    "    batch_size=batch_size, shuffle=True,\n",
    "    num_workers=workers, pin_memory=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10(root='data', train=False, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])),\n",
    "    batch_size=batch_size, shuffle=False,\n",
    "    num_workers=workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq_CNN(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, group, stride=1, padding=0, bias=True,depth = False):\n",
    "        super(Seq_CNN, self).__init__()\n",
    "\n",
    "        def hook_fn_backward_variable_cnn_u(grad):\n",
    "            grad.mul_(self.grad_mask_u)    \n",
    "        def hook_fn_backward_variable_cnn_v(grad):\n",
    "            grad.mul_(self.grad_mask_v)    \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.group_in = int(np.ceil(in_channels/group))\n",
    "        self.group_out = int(np.ceil(out_channels/group))\n",
    "        self.kernel_size = kernel_size\n",
    "        self.group = group\n",
    "        self.stride = stride\n",
    "        self.padding = padding \n",
    "        self.depth=depth\n",
    "        \n",
    "        if self.depth:\n",
    "            base_u = torch.Tensor(self.group_out,1,kernel_size,kernel_size)\n",
    "            base_v = torch.Tensor(self.group_out,1,kernel_size,kernel_size)            \n",
    "        else:\n",
    "            base_u = torch.Tensor(self.group_out,self.group_in,kernel_size,kernel_size)\n",
    "            base_v = torch.Tensor(self.group_out,self.group_out,kernel_size,kernel_size)\n",
    "        nn.init.kaiming_uniform_(base_u, a=math.sqrt(5))\n",
    "        nn.init.kaiming_uniform_(base_v, a=math.sqrt(5))\n",
    "\n",
    "        self.param_u = nn.Parameter(base_u)\n",
    "        self.param_v = nn.Parameter(base_v)\n",
    "\n",
    "        self.alpha = nn.Parameter(torch.tensor(0.5))\n",
    "        self.beta = nn.Parameter(torch.tensor(0.5))\n",
    "\n",
    "        if bias:\n",
    "            self.bias_u = nn.Parameter(torch.Tensor(self.group_out))\n",
    "            self.bias_v = nn.Parameter(torch.Tensor(self.group_out))\n",
    "        else:\n",
    "            self.register_parameter('bias_u', None)\n",
    "            self.register_parameter('bias_v', None)\n",
    "            self.reset_parameters()\n",
    "\n",
    "\n",
    "    def ode_compute(self, x, h):\n",
    "        return self.alpha*F.relu(x+h)-self.beta*h\n",
    "      \n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        #nn.init.kaiming_uniform_(self.kernel_u, a=math.sqrt(5))\n",
    "        #nn.init.kaiming_uniform_(self.kernel_v, a=math.sqrt(5))\n",
    "        if self.bias_u is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.param_u)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias_u, -bound, bound)\n",
    "            nn.init.uniform_(self.bias_v, -bound, bound)\n",
    "\n",
    "    def forward(self,x,h=None):\n",
    "        x_dup = torch.cat([x,x],dim=1)[:,:self.group_in*self.group]\n",
    "        #x_dup = x_dup[:,self.perm]\n",
    "        split = self.group_in\n",
    "        x_pads = torch.split(x_dup, split,dim=1)\n",
    "        outs = []\n",
    "        for xp in x_pads:\n",
    "            if len(outs)==0:\n",
    "                if self.depth:\n",
    "                    outs.append(F.conv2d(xp, self.param_u,self.bias_u,self.stride,self.padding,groups=self.group_in))\n",
    "                else:\n",
    "                    outs.append(F.conv2d(xp, self.param_u,self.bias_u,self.stride,self.padding,groups=1))\n",
    "            else:\n",
    "                if self.depth:\n",
    "                    o_c = F.conv2d(xp,self.param_u,self.bias_u,self.stride,self.padding,groups=self.group_in)\n",
    "                    o_l = F.conv2d(outs[-1],self.param_v,self.bias_v,1,(self.kernel_size-1)//2,groups=self.group_out)\n",
    "                else:\n",
    "                    o_c = F.conv2d(xp,self.param_u,self.bias_u,self.stride,self.padding)\n",
    "                    o_l = F.conv2d(outs[-1],self.param_v,self.bias_v,1,(self.kernel_size-1)//2)\n",
    "                outs.append(self.ode_compute(o_c,o_l)+o_l)\n",
    "        cout = torch.cat(outs,dim=1)[:,:self.out_channels]\n",
    "        return cout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseBlock(nn.Module):\n",
    "    alpha = 1\n",
    "\n",
    "    def __init__(self, input_channel, output_channel, t = 6, downsample = False):\n",
    "        \"\"\"\n",
    "            t:  expansion factor, t*input_channel is channel of expansion layer\n",
    "            alpha:  width multiplier, to get thinner models\n",
    "            rho:    resolution multiplier, to get reduced representation\n",
    "        \"\"\" \n",
    "        super(BaseBlock, self).__init__()\n",
    "        self.stride = 2 if downsample else 1\n",
    "        self.downsample = downsample\n",
    "        self.shortcut = (not downsample) and (input_channel == output_channel) \n",
    "\n",
    "        # apply alpha\n",
    "        input_channel = int(self.alpha * input_channel)\n",
    "        output_channel = int(self.alpha * output_channel)\n",
    "        \n",
    "        # for main path:\n",
    "        c  = t * input_channel\n",
    "        # 1x1   point wise conv\n",
    "        #self.conv1 = nn.Conv2d(input_channel, c, kernel_size = 1, bias = False)\n",
    "        self.conv1 = Seq_CNN(input_channel,c,kernel_size=1,group=3,stride=1,padding=0,bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(c)\n",
    "        # 3x3   depth wise conv\n",
    "        self.conv2 = nn.Conv2d(c, c, kernel_size = 3, stride = self.stride, padding = 1, groups = c, bias = False)\n",
    "        #self.conv2 = Seq_CNN(c,c ,kernel_size=3,group=3,stride=self.stride,padding=1,bias=False,depth=True) \n",
    "        self.bn2 = nn.BatchNorm2d(c)\n",
    "        # 1x1   point wise conv\n",
    "        if input_channel>0:\n",
    "        #self.conv3 = nn.Conv2d(c, output_channel, kernel_size = 1, bias = False)\n",
    "            self.conv3 = Seq_CNN(c,output_channel,kernel_size=1,group=4,stride=1,padding=0,bias=False)\n",
    "        else:\n",
    "            self.conv3 = nn.Conv2d(c, output_channel, kernel_size = 1, bias = False)\n",
    "        self.bn3 = nn.BatchNorm2d(output_channel)\n",
    "        \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # main path\n",
    "        x = F.relu6(self.bn1(self.conv1(inputs)), inplace = True)\n",
    "        x = F.relu6(self.bn2(self.conv2(x)), inplace = True)\n",
    "        x = self.bn3(self.conv3(x))\n",
    "\n",
    "        # shortcut path\n",
    "        x = x + inputs if self.shortcut else x\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self, output_size, alpha = 1):\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # first conv layer \n",
    "        self.conv0 = nn.Conv2d(3, int(32*alpha), kernel_size = 3, stride = 1, padding = 1, bias = False)\n",
    "        self.bn0 = nn.BatchNorm2d(int(32*alpha))\n",
    "\n",
    "        # build bottlenecks\n",
    "        BaseBlock.alpha = alpha\n",
    "        self.bottlenecks = nn.Sequential(\n",
    "            BaseBlock(32, 16, t = 1, downsample = False),\n",
    "            BaseBlock(16, 24, downsample = False),\n",
    "            BaseBlock(24, 24),\n",
    "            BaseBlock(24, 32, downsample = False),\n",
    "            BaseBlock(32, 32),\n",
    "            BaseBlock(32, 32),\n",
    "            BaseBlock(32, 64, downsample = True),\n",
    "            BaseBlock(64, 64),\n",
    "            BaseBlock(64, 64),\n",
    "            BaseBlock(64, 64),\n",
    "            BaseBlock(64, 96, downsample = False),\n",
    "            BaseBlock(96, 96),\n",
    "            BaseBlock(96, 96),\n",
    "            BaseBlock(96, 160, downsample = True),\n",
    "            BaseBlock(160, 160),\n",
    "            BaseBlock(160, 160),\n",
    "            BaseBlock(160, 320, downsample = False))\n",
    "\n",
    "        # last conv layers and fc layer\n",
    "        self.conv1 = nn.Conv2d(int(320*alpha), 1280, kernel_size = 1, bias = False)\n",
    "        #self.conv1 = Seq_CNN(int(320*alpha),1280,kernel_size=1,group=4,stride=1,padding=0,bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(1280)\n",
    "        self.fc = nn.Linear(1280, output_size)\n",
    "\n",
    "        # weights init\n",
    "        self.weights_init()\n",
    "\n",
    "\n",
    "    def weights_init(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        # first conv layer\n",
    "        x = F.relu6(self.bn0(self.conv0(inputs)), inplace = True)\n",
    "        # assert x.shape[1:] == torch.Size([32, 32, 32])\n",
    "\n",
    "        # bottlenecks\n",
    "        x = self.bottlenecks(x)\n",
    "        # assert x.shape[1:] == torch.Size([320, 8, 8])\n",
    "\n",
    "        # last conv layer\n",
    "        x = F.relu6(self.bn1(self.conv1(x)), inplace = True)\n",
    "        # assert x.shape[1:] == torch.Size([1280,8,8])\n",
    "\n",
    "        # global pooling and fc (in place of conv 1x1 in paper)\n",
    "        x = F.adaptive_avg_pool2d(x, 1)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_rnn(model,device, train_loader,test_loader, optimizer,criterion, epoch, eval_mode='on'):\n",
    "    global best\n",
    "    model.train()\n",
    "    final_cal = nn.Sigmoid()\n",
    "    train_error_logs = []\n",
    "    t1 = datetime.now()\n",
    "    test_loss_log = []\n",
    "    test_acc_log = []\n",
    "    train_corrects = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output_train = model(data)\n",
    "        loss = criterion(output_train, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pred_train = output_train.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "        train_corrects += pred_train.eq(target.view_as(pred_train)).sum().item()\n",
    "        train_error_logs.append(loss.item())\n",
    "        \n",
    "        if (batch_idx+0) % 20 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tTime: {:.2f}'.format(\n",
    "                epoch, (batch_idx+0)* len(target), len(train_loader.dataset),\n",
    "                100. * (batch_idx+0) / len(train_loader), loss,(datetime.now()-t1).total_seconds()))\n",
    "\n",
    "        \n",
    "    print('Train Epoch: {} Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "                epoch, train_corrects, len(train_loader.dataset),\n",
    "                100. * train_corrects / len(train_loader.dataset)))\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct_test = 0\n",
    "    with torch.no_grad():\n",
    "        for data_test, target_test in test_loader:\n",
    "            data_test, target_test = data_test.to(device), target_test.to(device)\n",
    "            output_test = model(data_test)\n",
    "            test_loss += criterion(output_test, target_test).item()  # sum up batch loss\n",
    "            pred_test = output_test.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct_test += pred_test.eq(target_test.view_as(pred_test)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_acc_log.append(correct_test/len(test_loader.dataset))\n",
    "    test_loss_log.append(test_loss)\n",
    "    print('Test set: Average loss: {:.8f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "            test_loss, correct_test, len(test_loader.dataset),\n",
    "            100. * correct_test / len(test_loader.dataset)))\n",
    "    if correct_test > best:\n",
    "        best = correct_test\n",
    "        torch.save(model,\"mobilenetv2.mdl\")\n",
    "        print(best,\"saved\")\n",
    "    return train_error_logs,test_loss_log,test_acc_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4716740326306993\n"
     ]
    }
   ],
   "source": [
    "model = MobileNetV2(10, alpha = 1).to(device)\n",
    "#model = vgg.vgg11().cuda()\n",
    "print(1-sum([torch.numel(li) for li in model.parameters()])/2237770)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function (criterion) and pptimizer\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr,\n",
    "                                momentum=momentum,\n",
    "                                weight_decay=weight_decay)\n",
    "best=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/50000 (0%)]\tLoss: 2.329591\tTime: 0.64\n",
      "Train Epoch: 0 [2560/50000 (5%)]\tLoss: 2.534220\tTime: 7.03\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-3254756ba2bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0madjust_learning_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtrain_error\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_error\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_acc_this\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_eval_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'on'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-95329ec0d75c>\u001b[0m in \u001b[0;36mtrain_eval_rnn\u001b[0;34m(model, device, train_loader, test_loader, optimizer, criterion, epoch, eval_mode)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mpred_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# get the index of the max log-probability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mtrain_corrects\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpred_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     99\u001b[0m                     \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmomentum\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                     \u001b[0mparam_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;34m'momentum_buffer'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparam_state\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'momentum_buffer'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0mrelevant_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverrides\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_time= []\n",
    "train_loss = []\n",
    "train_losses.append(train_loss)\n",
    "tests = []\n",
    "test_acc = []\n",
    "test_results_exp.append(tests)\n",
    "test_accs.append(test_acc)\n",
    "ratio = 0\n",
    "for epoch in range(0,600):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    t1 = datetime.now()\n",
    "    train_error,test_error,test_acc_this = train_eval_rnn(model,device, train_loader,val_loader, optimizer,criterion,epoch,'on')\n",
    "    train_loss.extend(train_error)\n",
    "    tests.extend(test_error)\n",
    "    test_acc.extend(test_acc_this)\n",
    "    train_time.append((datetime.now()-t1).total_seconds())\n",
    "    print((datetime.now()-t1).total_seconds())\n",
    "    #tests.append(test_rnn(model, device, test_loader,criterion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    \"\"\"\n",
    "    This function is taken from the original tf repo.\n",
    "    It ensures that all layers have a channel number that is divisible by 8\n",
    "    It can be seen here:\n",
    "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "    :param v:\n",
    "    :param divisor:\n",
    "    :param min_value:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "def conv_3x3_bn(inp, oup, stride):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.ReLU6(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "def conv_1x1_bn(inp, oup):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.ReLU6(inplace=True)\n",
    "    )\n",
    "\n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    \"\"\"\n",
    "    This function is taken from the original tf repo.\n",
    "    It ensures that all layers have a channel number that is divisible by 8\n",
    "    It can be seen here:\n",
    "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "    :param v:\n",
    "    :param divisor:\n",
    "    :param min_value:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, expand_ratio,celltype):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        hidden_dim = round(inp * expand_ratio)\n",
    "        self.identity = stride == 1 and inp == oup\n",
    "\n",
    "        if expand_ratio == 1:\n",
    "            self.conv = nn.Sequential(\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                # pw\n",
    "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False) if celltype==\"cnn\" else Seq_CNN(inp,hidden_dim,kernel_size=1,group=3,stride=1,padding=0,bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "                # pw-linear\n",
    "                #nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False) if celltype==\"cnn\" else Seq_CNN(hidden_dim,oup,kernel_size=1,group=4,stride=1,padding=0,bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.identity:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self, num_classes=1000, width_mult=1.):\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        # setting of inverted residual blocks\n",
    "        self.cfgs = [\n",
    "            # t, c, n, s\n",
    "            [1,  16, 1, 1],\n",
    "            [6,  24, 2, 2],\n",
    "            [6,  32, 3, 2],\n",
    "            [6,  64, 4, 2],\n",
    "            [6,  96, 3, 1],\n",
    "            [6, 160, 3, 2],\n",
    "            [6, 320, 1, 1],\n",
    "        ]\n",
    "\n",
    "        # building first layer\n",
    "        input_channel = _make_divisible(32 * width_mult, 4 if width_mult == 0.1 else 8)\n",
    "        layers = [conv_3x3_bn(3, input_channel, 2)]\n",
    "        # building inverted residual blocks\n",
    "        block = InvertedResidual\n",
    "        total = 0\n",
    "        for t, c, n, s in self.cfgs:\n",
    "            output_channel = _make_divisible(c * width_mult, 4 if width_mult == 0.1 else 8)\n",
    "            for i in range(n):\n",
    "                if total<=4:\n",
    "                    layers.append(block(input_channel, output_channel, s if i == 0 else 1, t,\"cnn\"))\n",
    "                else:\n",
    "                    layers.append(block(input_channel, output_channel, s if i == 0 else 1, t,\"seq\"))\n",
    "                input_channel = output_channel\n",
    "                total+=1\n",
    "        print(total)\n",
    "        self.features = nn.Sequential(*layers)\n",
    "        # building last several layers\n",
    "        output_channel = _make_divisible(1280 * width_mult, 4 if width_mult == 0.1 else 8) if width_mult > 1.0 else 1280\n",
    "        self.conv = conv_1x1_bn(input_channel, output_channel)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Linear(output_channel, num_classes)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "0.2955291947894245 2469080\n"
     ]
    }
   ],
   "source": [
    "model = MobileNetV2()\n",
    "#model = vgg.vgg11().cuda()\n",
    "print(1-sum([torch.numel(li) for li in model.parameters()])/3504872,sum([torch.numel(li) for li in model.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MobileNetV2(\n",
       "  (features): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6(inplace=True)\n",
       "        (3): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6(inplace=True)\n",
       "        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU6(inplace=True)\n",
       "        (6): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6(inplace=True)\n",
       "        (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "        (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU6(inplace=True)\n",
       "        (6): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6(inplace=True)\n",
       "        (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "        (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU6(inplace=True)\n",
       "        (6): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6(inplace=True)\n",
       "        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU6(inplace=True)\n",
       "        (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Seq_CNN()\n",
       "        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6(inplace=True)\n",
       "        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU6(inplace=True)\n",
       "        (6): Seq_CNN()\n",
       "        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Seq_CNN()\n",
       "        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6(inplace=True)\n",
       "        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU6(inplace=True)\n",
       "        (6): Seq_CNN()\n",
       "        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Seq_CNN()\n",
       "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6(inplace=True)\n",
       "        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU6(inplace=True)\n",
       "        (6): Seq_CNN()\n",
       "        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Seq_CNN()\n",
       "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6(inplace=True)\n",
       "        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU6(inplace=True)\n",
       "        (6): Seq_CNN()\n",
       "        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Seq_CNN()\n",
       "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6(inplace=True)\n",
       "        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU6(inplace=True)\n",
       "        (6): Seq_CNN()\n",
       "        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Seq_CNN()\n",
       "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6(inplace=True)\n",
       "        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU6(inplace=True)\n",
       "        (6): Seq_CNN()\n",
       "        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (12): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Seq_CNN()\n",
       "        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6(inplace=True)\n",
       "        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU6(inplace=True)\n",
       "        (6): Seq_CNN()\n",
       "        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (13): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Seq_CNN()\n",
       "        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6(inplace=True)\n",
       "        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU6(inplace=True)\n",
       "        (6): Seq_CNN()\n",
       "        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (14): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Seq_CNN()\n",
       "        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6(inplace=True)\n",
       "        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
       "        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU6(inplace=True)\n",
       "        (6): Seq_CNN()\n",
       "        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (15): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Seq_CNN()\n",
       "        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6(inplace=True)\n",
       "        (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "        (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU6(inplace=True)\n",
       "        (6): Seq_CNN()\n",
       "        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (16): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Seq_CNN()\n",
       "        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6(inplace=True)\n",
       "        (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "        (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU6(inplace=True)\n",
       "        (6): Seq_CNN()\n",
       "        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (17): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Seq_CNN()\n",
       "        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6(inplace=True)\n",
       "        (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "        (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU6(inplace=True)\n",
       "        (6): Seq_CNN()\n",
       "        (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU6(inplace=True)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (classifier): Linear(in_features=1280, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
